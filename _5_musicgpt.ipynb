{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asampat3090/musicalai/blob/main/_5_musicgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHR-Wkc_NdCQ"
      },
      "source": [
        "# MusicGPT (Step 2)\n",
        "In this notebook we will attempt to use the GPT-2 decoder-only architecture to generate audio sequences that continue the input, that is we will ask it to predict the next audio token in an auto-regressive way. We expect this will not be as good as architectures like [MusicLM](https://google-research.github.io/seanet/musiclm/examples/) or [MusicGen](https://ai.honu.io/papers/musicgen/) where text-conditioning is used, but we hope it clarifies how a simple model can be built to continue audio tokens. This will be trained and evaluated on open-source audio sample datasets.\n",
        "\n",
        "Basic implementation of the GPT-2 architecture. We will be modifying this to take in sequential tokenized audio inputs, but will implement the architecture first to recap the structure and validate the choices made. We will revisit the shape of `x` and hyperparameters present in `config` (i.e. should we use the same params as GPT-2?)\n",
        "\n",
        "Full Code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR0eLO5YNdCT"
      },
      "source": [
        "## Building Blocks\n",
        "\n",
        "We will start with the basic model building blocks of the architecture of the transformer architecture with appropriate hyperparameters which we will update when we need to shanve the input and embedding type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sn3qivdNdCT"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "import dataclasses as dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmxEWW0KNdCU"
      },
      "source": [
        "### LayerNorm\n",
        "\n",
        "Unlike batch norm layer norm does not impose constraints on the size of the mini-batch and can be used in a pure online way batch size 1. More specifically we compute the mean and standard deviation over all hidden units in a layer (rather than across across all N in the batch in batch norm)\n",
        "\n",
        "$\\mu^l = \\frac{1}{H}\\sum_{i=1}^{H} a_i^l$\n",
        "\n",
        "$\\sigma^l=\\sqrt{\\frac{1}{H}\\sum_{i=1}^{H}(a_i^l -\\mu^l)^2}$\n",
        "\n",
        "More details: https://paperswithcode.com/method/layer-normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNelrvJWNdCV"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"this module is already defined in pytorch but we add a bias term\"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.params(torch.zeroes(ndim)) if bias else None\n",
        "\n",
        "    def forward(self,input):\n",
        "        return F.layer_norm(input,  self.weight.shape,self.weight,self.bias,1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDw_zvtSNdCV"
      },
      "source": [
        "### Causal Self Attention (Masked Multihead Attention)\n",
        "\n",
        "Here we are using the GPT-2 architecture which is a decoder only network that uses causal self-attention, that is the queries, keys and values all come from the same input sequence and it is causal because it will only consider all tokens up to and including the current context but ignore anything after that. At each subsequent step it will consider one more token. This decoder is auto-regressive so will step through token by token to generate a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jho32JkiNdCV"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config): # config is a dict of values to be defined\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 # n_embd = d_e = d_k = d_q = d_v\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        # define params needed for downstream processing\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout # probability of dropout\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() #batch size, sequence length, embedding dim (n_embd)\n",
        "\n",
        "        # calculate query, key, value for all heads in batch\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        # move head forward to be the batch dim -> note embeddings are split across heads for multi-head\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; (B, nh, T, hs) x (B, nh, T, hs) -> (B, nh, T, T)\n",
        "        attn = (q @ k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1))) # (B,nh,T,hs) X (B,nh,hs,T) -> (B,nh,T,T)\n",
        "        attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # fill in top right corner of matrix (ignore subsequent tokens)\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "        y = attn @ v # (B,nh,T,T) x (B,nh,T,hs) -> (B,nh,T,hs)\n",
        "        # reassemble multiple heads into one (B,nh,T,hs) -> (B,T,C)\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,C) # combine nh and hs to reform C\n",
        "        # apply final projection and dropout\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGzp2VcDNdCW"
      },
      "source": [
        "We will run a basic test to confirm this works, using a `1x3x3` tensor, that is `B=1`, `T=3` and `C=3`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcjlIGB7NdCW",
        "outputId": "572b19ba-5b97-4668-9275-d9b37a7fa8d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[[0.2,-0.1,-0.9],[0.1,-0.4,-0.9],[0.5,-0.3,-0.9]]]) # 1 x 3 x 3\n",
        "config = {\n",
        "    \"n_head\": 3,\n",
        "    \"n_embd\": 3,\n",
        "    \"block_size\": 3,\n",
        "    \"bias\": False,\n",
        "    \"dropout\": 0.5\n",
        "}\n",
        "class DotDict(dict):\n",
        "    \"\"\"\n",
        "    a dictionary that supports dot notation\n",
        "    as well as dictionary access notation\n",
        "    usage: d = DotDict() or d = DotDict({'val1':'first'})\n",
        "    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n",
        "    get attributes: d.val2 or d['val2']\n",
        "    \"\"\"\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "test_CSA = CausalSelfAttention(DotDict(config))\n",
        "print(test_CSA.forward(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCK1OUF7NdCX"
      },
      "source": [
        "### MLP - Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0c0-3M1NdCX"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjToqCo9NdCX"
      },
      "source": [
        "### Composite Block for GPT-2\n",
        "\n",
        "Now we can compose the block used in GPT-2. The decoder block takes both positional and token embeddings, layer normalizes, passes through the masked self-attention, layer normalizes, and finally runs through an MLP layer. More details are illustrated well here: http://jalammar.github.io/illustrated-gpt2/\n",
        "\n",
        "In the final model we can repeat this block `n_layer` number of times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMR3U89VNdCX"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYLMl8_ZNdCX"
      },
      "source": [
        "### GPT Model (adapted for music) - MusicGPT\n",
        "\n",
        "Here we will adapt the nanoGPT aka GPT-2 architecture to work with encoded audio with multiple types of indices. The goal is still to predict the next sequence in time, though we will have to be clever with the encoding and decoding of these encodings.\n",
        "\n",
        "We will also need to come up with a \"word token embedding\" and \"word positional embedding\" which works for audio. We will call these \"audio token embedding\" and \"audio positional embedding\" which will be attempt to find an equivalent representation for each time step in the audio (similar to words or other tokens in language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF2N5IeVNdCX"
      },
      "outputs": [],
      "source": [
        "# class MusicGPT(nn.Module):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XUhPrwyNdCX"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6utQ-jhANdCY"
      },
      "source": [
        "## Training GPT with Music Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNtUA32vNdCY"
      },
      "outputs": [],
      "source": [
        "# https://github.com/karpathy/nanoGPT/blob/master/train.py\n",
        "# http://jalammar.github.io/illustrated-gpt2/ - can this even be used for music generation?"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}